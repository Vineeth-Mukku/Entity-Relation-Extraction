{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from nltk.wsd import lesk\n",
    "import networkx as nx\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "class CorpusReader:\n",
    "    def __init__(self):\n",
    "        file = open(\"train.txt\", \"r\")\n",
    "        f = file.readlines()\n",
    "        lst = []\n",
    "        for line in f:\n",
    "            x = line.strip()\n",
    "            if x != \"\":\n",
    "                lst.append(x)\n",
    "        inputList = []\n",
    "        for i in range(0, len(lst), 2):\n",
    "            newlst = []\n",
    "            sent = lst[i].split(\"\\t\")[1][2 : -2]\n",
    "            stind_e1 = sent.find(\"<e1>\")\n",
    "            endind_e1 = sent.find(\"</e1>\")\n",
    "            e1 = sent[stind_e1 + 4 : endind_e1]\n",
    "            stind_e2 = sent.find(\"<e2>\")\n",
    "            endind_e2 = sent.find(\"</e2>\")\n",
    "            e2 = sent[stind_e2 + 4 : endind_e2]\n",
    "            newlst.append(sent[:stind_e1] + sent[stind_e1+5:endind_e1] + sent[endind_e1+6:stind_e2] + sent[stind_e2+5:endind_e2] + sent[endind_e2+6:])\n",
    "            newlst.append(e1.strip())\n",
    "            newlst.append(e2.strip())\n",
    "            newlst.append(sent[endind_e1+6:stind_e2-1])\n",
    "            toplst = []\n",
    "            top = stind_e1 - 2\n",
    "            if top >= 0:\n",
    "                while top >= 0 and sent[top] != \" \":\n",
    "                    top -= 1\n",
    "                bef = sent[top+1:stind_e1-1]\n",
    "            else:\n",
    "                bef = \"\"\n",
    "            toplst.append(bef)\n",
    "            top = endind_e2 + 6\n",
    "            if top < len(sent):\n",
    "                while top < len(sent) and sent[top] != \" \":\n",
    "                    top += 1\n",
    "                aftr = sent[endind_e2+6:top]\n",
    "            else:\n",
    "                aftr = \"\"\n",
    "            toplst.append(aftr)\n",
    "            newlst.append(toplst)\n",
    "            if lst[i + 1] == \"no_relation\":\n",
    "                newlst.append(None)\n",
    "                newlst.append(\"no_relation\")\n",
    "            else:\n",
    "                i1 = lst[i + 1].find(\":\")\n",
    "                i2 = lst[i + 1].find(\"(\")\n",
    "                if(lst[i + 1][i2 + 2] == '1'):\n",
    "                    newlst.append(True)\n",
    "                else:\n",
    "                    newlst.append(False)\n",
    "                newlst.append(lst[i + 1][i1 + 1 : i2])\n",
    "            inputList.append(newlst)\n",
    "        self.inputList = inputList\n",
    "        file = open(\"test.txt\", \"r\")\n",
    "        f = file.readlines()\n",
    "        lst = []\n",
    "        for line in f:\n",
    "            x = line.strip()\n",
    "            if x != \"\":\n",
    "                lst.append(x)\n",
    "        inputList = []\n",
    "        for i in range(0, len(lst), 2):\n",
    "            newlst = []\n",
    "            sent = lst[i].split(\"\\t\")[1][2 : -2]\n",
    "            stind_e1 = sent.find(\"<e1>\")\n",
    "            endind_e1 = sent.find(\"</e1>\")\n",
    "            e1 = sent[stind_e1 + 4 : endind_e1]\n",
    "            stind_e2 = sent.find(\"<e2>\")\n",
    "            endind_e2 = sent.find(\"</e2>\")\n",
    "            e2 = sent[stind_e2 + 4 : endind_e2]\n",
    "            newlst.append(sent[:stind_e1] + sent[stind_e1+5:endind_e1] + sent[endind_e1+6:stind_e2] + sent[stind_e2+5:endind_e2] + sent[endind_e2+6:])\n",
    "            newlst.append(e1.strip())\n",
    "            newlst.append(e2.strip())\n",
    "            newlst.append(sent[endind_e1+6:stind_e2-1])\n",
    "            toplst = []\n",
    "            top = stind_e1 - 2\n",
    "            if top >= 0:\n",
    "                while top >= 0 and sent[top] != \" \":\n",
    "                    top -= 1\n",
    "                bef = sent[top+1:stind_e1-1]\n",
    "            else:\n",
    "                bef = \"\"\n",
    "            toplst.append(bef)\n",
    "            top = endind_e2 + 6\n",
    "            if top < len(sent):\n",
    "                while top < len(sent) and sent[top] != \" \":\n",
    "                    top += 1\n",
    "                aftr = sent[endind_e2+6:top]\n",
    "            else:\n",
    "                aftr = \"\"\n",
    "            toplst.append(aftr)\n",
    "            newlst.append(toplst)\n",
    "            if lst[i + 1] == \"no_relation\":\n",
    "                newlst.append(None)\n",
    "                newlst.append(\"no_relation\")\n",
    "            else:\n",
    "                i1 = lst[i + 1].find(\":\")\n",
    "                i2 = lst[i + 1].find(\"(\")\n",
    "                if(lst[i + 1][i2 + 2] == '1'):\n",
    "                    newlst.append(True)\n",
    "                else:\n",
    "                    newlst.append(False)\n",
    "                newlst.append(lst[i + 1][i1 + 1 : i2])\n",
    "            inputList.append(newlst)\n",
    "        self.inputList2 = inputList\n",
    "\n",
    "# inputList is a list of [Sentence, e1, e2, Sentence b/w entities, before and after word of e1 n e2, Direction of relation, Name of relation] lists\n",
    "\n",
    "cr = CorpusReader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts test sentence into the form we want to process our model with\n",
    "\n",
    "def createlist_ts(sent):\n",
    "    testList = []\n",
    "    newlst = []\n",
    "    sent = sent[1 : -1].strip()\n",
    "    stind_e1 = sent.find(\"<e1>\")\n",
    "    endind_e1 = sent.find(\"</e1>\")\n",
    "    e1 = sent[stind_e1 + 4 : endind_e1]\n",
    "    stind_e2 = sent.find(\"<e2>\")\n",
    "    endind_e2 = sent.find(\"</e2>\")\n",
    "    e2 = sent[stind_e2 + 4 : endind_e2]\n",
    "    newlst.append(sent[:stind_e1] + sent[stind_e1+5:endind_e1] + sent[endind_e1+6:stind_e2] + sent[stind_e2+5:endind_e2] + sent[endind_e2+6:])\n",
    "    newlst.append(e1.strip())\n",
    "    newlst.append(e2.strip())\n",
    "    newlst.append(sent[endind_e1+6:stind_e2-1])\n",
    "    toplst = []\n",
    "    top = stind_e1 - 2\n",
    "    if top >= 0:\n",
    "        while top >= 0 and sent[top] != \" \":\n",
    "            top -= 1\n",
    "        bef = sent[top+1:stind_e1-1]\n",
    "    else:\n",
    "        bef = \"\"\n",
    "    toplst.append(bef)\n",
    "    top = endind_e2 + 6\n",
    "    if top < len(sent):\n",
    "        while top < len(sent) and sent[top] != \" \":\n",
    "            top += 1\n",
    "        aftr = sent[endind_e2+6:top]\n",
    "    else:\n",
    "        aftr = \"\"\n",
    "    toplst.append(aftr)\n",
    "    newlst.append(toplst)\n",
    "    testList.append(newlst)\n",
    "    return testList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "# e1Tokens is a list of e1 Tokens lists and e2Tokens is a list of e2 Tokens lists\n",
    "\n",
    "def entities_tokens(list1):\n",
    "    e1Tokens = []\n",
    "    e2Tokens = []\n",
    "    for ind in list1:\n",
    "        e1Tokens.append(word_tokenize(ind[1]))\n",
    "        e2Tokens.append(word_tokenize(ind[2]))\n",
    "    return e1Tokens, e2Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1Tokens_train, e2Tokens_train = entities_tokens(cr.inputList)\n",
    "e1Tokens_test, e2Tokens_test = entities_tokens(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas is a list of sentence's lemmas lists\n",
    "\n",
    "def lemmas_sentences(list1):\n",
    "    lemmas = []\n",
    "    for ind in list1:\n",
    "        sent = ind[0]\n",
    "        lm = nltk.word_tokenize(sent)\n",
    "        nl = []\n",
    "        for r in lm:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            nl.append(lemmatizer.lemmatize(r))\n",
    "        lemmas.append(nl)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos is a list of POS Tags lists\n",
    "\n",
    "def pos_sentences(list1):\n",
    "    pos = []\n",
    "    for ind in list1:\n",
    "        sent = ind[0]\n",
    "        lm = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        nl = []\n",
    "        for r in lm:\n",
    "            tag = r[1]\n",
    "            nl.append(tag)\n",
    "        pos.append(nl)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deps is a list of dependency b/w 2 tokens and the root of it lists\n",
    "\n",
    "def deps_sentences(list1):\n",
    "    deps = []\n",
    "    for ind in list1:\n",
    "        sent = ind[0] \n",
    "        doc = nlp(sent)\n",
    "        lm = []\n",
    "        for tkn in doc:\n",
    "            lm.append([tkn.dep_, tkn.head.text])\n",
    "        deps.append(lm)\n",
    "    return deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps_train = deps_sentences(cr.inputList)\n",
    "deps_test = deps_sentences(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypernyms, hyponyms, meronyms and holonyms are list of lists\n",
    "\n",
    "def hhmh_sentences(list1):\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    meronyms = []\n",
    "    holonyms = [] \n",
    "    for ind in list1:\n",
    "        sent = ind[0]\n",
    "        tkns = word_tokenize(sent)\n",
    "        tmp1 = []\n",
    "        tmp2 = []\n",
    "        tmp3 = []\n",
    "        tmp4 = []\n",
    "        for tkn in tkns:\n",
    "            syn = lesk(tkns, tkn)\n",
    "            if syn is not None:\n",
    "                for hypernym in syn.hypernyms():\n",
    "                    tmp1.append(hypernym)\n",
    "                for hyponym in syn.hyponyms():\n",
    "                    tmp2.append(hyponym)\n",
    "                for meronym in syn.part_meronyms():\n",
    "                    tmp3.append(meronym)\n",
    "                for holonym in syn.part_holonyms():\n",
    "                    tmp4.append(holonym)\n",
    "        hypernyms.append(tmp1)\n",
    "        hyponyms.append(tmp2)\n",
    "        meronyms.append(tmp3)\n",
    "        holonyms.append(tmp4)\n",
    "    return hypernyms, hyponyms, meronyms, holonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e1pos n e2pos are lists of pos tag lists of e1 n e2\n",
    "\n",
    "def e1pos_e2pos(list1, list2):\n",
    "    e1pos = []\n",
    "    e2pos = []\n",
    "\n",
    "    for tkns in list1:\n",
    "        lm = nltk.pos_tag(tkns)\n",
    "        nl = []\n",
    "        for r in lm:\n",
    "            tag = r[1]\n",
    "            nl.append(tag)\n",
    "        e1pos.append(nl)\n",
    "\n",
    "    for tkns in list2:\n",
    "        lm = nltk.pos_tag(tkns)\n",
    "        nl = []\n",
    "        for r in lm:\n",
    "            tag = r[1]\n",
    "            nl.append(tag)\n",
    "        e2pos.append(nl)\n",
    "    return e1pos, e2pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1pos_train, e2pos_train = e1pos_e2pos(e1Tokens_train, e2Tokens_train)\n",
    "e1pos_test, e2pos_test = e1pos_e2pos(e1Tokens_test, e2Tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bwwords_pos is a list of pos tags of words b/w entities lists\n",
    "\n",
    "def bwwords_pos_entities(list1):\n",
    "    bwwords_pos = [] \n",
    "    for ind in list1:\n",
    "        sentbwwrds = ind[3]\n",
    "        lm = nltk.pos_tag(nltk.word_tokenize(sentbwwrds))\n",
    "        nl = []\n",
    "        for r in lm:\n",
    "            tag = r[1]\n",
    "            nl.append(tag)\n",
    "        bwwords_pos.append(nl)\n",
    "    return bwwords_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwwords_pos_train = bwwords_pos_entities(cr.inputList)\n",
    "bwwords_pos_test = bwwords_pos_entities(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noOfWords is a list of no of words b/w entities lists\n",
    "\n",
    "def noOfWords_entities(list1):\n",
    "    noOfWords = []\n",
    "    for ind in list1:\n",
    "        sent = ind[3]\n",
    "        noOfWords.append(len(sent.split(\" \")))\n",
    "    return noOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfWords_train = noOfWords_entities(cr.inputList)\n",
    "noOfWords_test = noOfWords_entities(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest Dependency Path\n",
    "\n",
    "def sdp_sentences(list1):\n",
    "    sdp = []\n",
    "\n",
    "    for ind in list1:\n",
    "        sent = ind[0]\n",
    "        doc = nlp(sent)\n",
    "        edges = []\n",
    "        for tkn in doc:\n",
    "            for chld in tkn.children:\n",
    "                edges.append(('{0}'.format(tkn.lower_), '{0}'.format(chld.lower_)))\n",
    "        graph = nx.Graph(edges)\n",
    "        entity1 = \"\"\n",
    "        entity2 = \"\"\n",
    "        ent1wds = ind[1].lower().split(\" \")\n",
    "        ent2wds = ind[2].lower().split(\" \")\n",
    "        for wds in ent1wds:\n",
    "            if wds in graph.nodes:\n",
    "                entity1 = wds\n",
    "                break\n",
    "        for wds in ent2wds:\n",
    "            if wds in graph.nodes:\n",
    "                entity2 = wds\n",
    "                break\n",
    "        if entity1 == \"\" or entity2 == \"\":\n",
    "            sdp.append([])\n",
    "        else:\n",
    "            try:\n",
    "                sdp.append(nx.shortest_path(graph, source = entity1, target = entity2))\n",
    "            except:\n",
    "                sdp.append([])\n",
    "    return sdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp_train = sdp_sentences(cr.inputList)\n",
    "sdp_test = sdp_sentences(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e1hypernyms and e2hypernyms are list of e1 n e2 hypernyms lists\n",
    "\n",
    "def e1e2_hypernyms(list1):\n",
    "    e1hypernyms = []\n",
    "    e2hypernyms = []\n",
    "\n",
    "    for ind in list1:\n",
    "        e1 = ind[1]\n",
    "        tkns = word_tokenize(e1)\n",
    "        tmp1 = []\n",
    "        for tkn in tkns:\n",
    "            syn = lesk(tkns, tkn)\n",
    "            if syn is not None:\n",
    "                hyp = syn.hypernyms()\n",
    "                if hyp:\n",
    "                    tmp1.append(hyp[0])\n",
    "        if not tmp1:\n",
    "            e1hypernyms.append(tmp1)\n",
    "        else: \n",
    "            e1hypernyms.append(tmp1[-1:])\n",
    "        e2 = ind[2]\n",
    "        tkns = word_tokenize(e2)\n",
    "        tmp1 = []\n",
    "        for tkn in tkns:\n",
    "            syn = lesk(tkns, tkn)\n",
    "            if syn is not None:\n",
    "                hyp = syn.hypernyms()\n",
    "                if hyp:\n",
    "                    tmp1.append(hyp[0])\n",
    "        if not tmp1:\n",
    "            e2hypernyms.append(tmp1)\n",
    "        else: \n",
    "            e2hypernyms.append(tmp1[-1:])\n",
    "    return e1hypernyms, e2hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1hypernyms_train, e2hypernyms_train = e1e2_hypernyms(cr.inputList)\n",
    "e1hypernyms_test, e2hypernyms_test = e1e2_hypernyms(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e1ner and e2ner is a list of ner tags of e1 n e2\n",
    "\n",
    "def e1e2_ner(list1):\n",
    "    e1ner = []\n",
    "    e2ner = []\n",
    "\n",
    "    for ind in list1:\n",
    "        sent = ind[1]\n",
    "        doc = nlp(sent)\n",
    "        lm = []\n",
    "        for tkn in doc.ents:\n",
    "            lm.append([tkn.label_])\n",
    "        e1ner.append(lm)\n",
    "        sent = ind[2]\n",
    "        doc = nlp(sent)\n",
    "        lm = []\n",
    "        for tkn in doc.ents:\n",
    "            lm.append([tkn.label_])\n",
    "        e2ner.append(lm)\n",
    "    return e1ner, e2ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1ner_train, e2ner_train = e1e2_ner(cr.inputList)\n",
    "e1ner_test, e2ner_test = e1e2_ner(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "def unique(list1): \n",
    "    unique_list = []  \n",
    "    for x in list1: \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "    return unique_list\n",
    "\n",
    "def list_to_string_original(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        newlst.append(\"-\".join(x))\n",
    "    return newlst\n",
    "\n",
    "def list_to_string(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        newlst.append(\"-\".join(x))\n",
    "    newlst.append(\"*****\")\n",
    "    return newlst\n",
    "\n",
    "def assign_indices(list1):\n",
    "    dic1 = {}\n",
    "    cnt = 0\n",
    "    for x in list1:\n",
    "        dic1[x] = cnt\n",
    "        cnt += 1\n",
    "    dic2 = {}\n",
    "    cnt = 0\n",
    "    for x in list1:\n",
    "        dic2[cnt] = x\n",
    "        cnt += 1\n",
    "    return dic1, dic2\n",
    "\n",
    "def listoflist_to_string_original(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        tmp = []\n",
    "        for i in x:\n",
    "            if len(i) > 0:\n",
    "                tmp.append(i[0])\n",
    "        newlst.append(\"-\".join(tmp))\n",
    "    return newlst\n",
    "\n",
    "def listoflist_to_string(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        tmp = []\n",
    "        for i in x:\n",
    "            if len(i) > 0:\n",
    "                tmp.append(i[0])\n",
    "        newlst.append(\"-\".join(tmp))\n",
    "    newlst.append(\"*****\")\n",
    "    return newlst\n",
    "\n",
    "def hypernyms_to_string_original(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        if not x:\n",
    "            strg = \"\"\n",
    "        else:\n",
    "            strg = x[0].name()\n",
    "        newlst.append(strg)\n",
    "    return newlst\n",
    "\n",
    "def hypernyms_to_string(list1):\n",
    "    newlst = []\n",
    "    for x in list1:\n",
    "        if not x:\n",
    "            strg = \"\"\n",
    "        else:\n",
    "            strg = x[0].name()\n",
    "        newlst.append(strg)\n",
    "    newlst.append(\"*****\")\n",
    "    return newlst\n",
    "\n",
    "def convert_to_string(dirtion, rel):\n",
    "    tmp = []\n",
    "    for x in range(len(dirtion)):\n",
    "        tmp.append(rel[x] + f\" {dirtion[x]}\")\n",
    "    return tmp\n",
    "\n",
    "def get_items(list1, tt):\n",
    "    e1 = []\n",
    "    for ind in list1:\n",
    "        e1.append(ind[tt])\n",
    "    return e1\n",
    "\n",
    "def get_integer_encoded(lst, dic):\n",
    "    integer_encoded = []\n",
    "    for x in lst:\n",
    "        if x not in dic.keys():\n",
    "            strp = dic[\"*****\"]\n",
    "        else:\n",
    "            strp = dic[x]\n",
    "        integer_encoded.append(strp)\n",
    "    return integer_encoded\n",
    "\n",
    "def get_onehot_encoded(lst1, lst2):\n",
    "    onehot_encoded = list()\n",
    "    for value in lst1:\n",
    "        letter = [0 for _ in range(len(lst2))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    return onehot_encoded\n",
    "\n",
    "def flatten_test_sentence():\n",
    "    xw_test = []\n",
    "    for ind in range(len(testList)):\n",
    "        tmp = []\n",
    "        for x in onehot_encoded_e1words_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e2words_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_bww_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e1_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e2_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_bwwords_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_befaft_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_noOfWords_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e1hyp_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e2hyp_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e1ner_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        for x in onehot_encoded_e1ner_test_sentence[ind]:\n",
    "            tmp.append(x)\n",
    "        xw_test.append(tmp)\n",
    "    return xw_test\n",
    "\n",
    "def construct_csr(list1):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list1[0])):\n",
    "            if list1[i][j] == 1:\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                data.append(1)\n",
    "\n",
    "    X_train = csr_matrix((data, (row, col)), shape=(len(list1), len(list1[0])))\n",
    "    return X_train\n",
    "\n",
    "def predict_y(feat_name):\n",
    "    if feat_name == \"*****\":\n",
    "        print(\"Sorry, our model is unable to predict the relation between the entities in the test sentence!\")\n",
    "    else:\n",
    "        lst = feat_name.split(\" \")\n",
    "        rel = lst[0]\n",
    "        dirc = lst[1]\n",
    "        if rel == \"no_relation\":\n",
    "            print(\"There is no relation between the entities in the test sentence!\")\n",
    "        else:\n",
    "            if dirc == 'True':\n",
    "                print(f\"The relation between entities in the test sentence is {rel}(e1, e2)\")\n",
    "            elif dirc == 'False':\n",
    "                print(f\"The relation between entities in the test sentence is {rel}(e2, e1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1 n e2 words\n",
    "# train\n",
    "\n",
    "e1_string = get_items(cr.inputList, 1)\n",
    "e2_string = get_items(cr.inputList, 2)    \n",
    "e1unique_string = unique(e1_string)\n",
    "e2unique_string = unique(e2_string)\n",
    "e1unique_string.append(\"*****\")\n",
    "e2unique_string.append(\"*****\")\n",
    "e1wordslist_to_int, e1wordsint_to_list = assign_indices(e1unique_string)\n",
    "e2wordslist_to_int, e2wordsint_to_list = assign_indices(e2unique_string)\n",
    "integer_encoded_e1words = [e1wordslist_to_int[s] for s in e1_string]\n",
    "integer_encoded_e2words = [e2wordslist_to_int[s] for s in e2_string]\n",
    "onehot_encoded_e1words = get_onehot_encoded(integer_encoded_e1words, e1unique_string)\n",
    "onehot_encoded_e2words = get_onehot_encoded(integer_encoded_e2words, e2unique_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1 n e2 words\n",
    "# test\n",
    "\n",
    "def onehotencoding_e1e2Words(list1):\n",
    "    e1_string_test = get_items(list1, 1)\n",
    "    e2_string_test = get_items(list1, 2)\n",
    "    integer_encoded_e1words_test = get_integer_encoded(e1_string_test, e1wordslist_to_int)\n",
    "    integer_encoded_e2words_test = get_integer_encoded(e2_string_test, e2wordslist_to_int)\n",
    "    onehot_encoded_e1words_test = get_onehot_encoded(integer_encoded_e1words_test, e1unique_string)\n",
    "    onehot_encoded_e2words_test = get_onehot_encoded(integer_encoded_e2words_test, e2unique_string)\n",
    "    return e1_string_test, e2_string_test, integer_encoded_e1words_test, integer_encoded_e2words_test, onehot_encoded_e1words_test, onehot_encoded_e2words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_string_test, e2_string_test, integer_encoded_e1words_test, integer_encoded_e2words_test, onehot_encoded_e1words_test, onehot_encoded_e2words_test = onehotencoding_e1e2Words(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of words b/w e1 n e2\n",
    "# train\n",
    "\n",
    "bw_words_string = get_items(cr.inputList, 3)   \n",
    "bwwords_unique_string = unique(bw_words_string)\n",
    "bwwords_unique_string.append(\"*****\")\n",
    "bwwlist_to_int, bwwint_to_list = assign_indices(bwwords_unique_string)\n",
    "integer_encoded_bww = [bwwlist_to_int[s] for s in bw_words_string]\n",
    "onehot_encoded_bww = get_onehot_encoded(integer_encoded_bww, bwwords_unique_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of words b/w e1 n e2\n",
    "# test\n",
    "\n",
    "def onehotencoding_betweenWords(list1):\n",
    "    bw_words_string_test = get_items(list1, 3)\n",
    "    integer_encoded_bww_test = get_integer_encoded(bw_words_string_test, bwwlist_to_int)\n",
    "    onehot_encoded_bww_test = get_onehot_encoded(integer_encoded_bww_test, bwwords_unique_string)\n",
    "    return bw_words_string_test, integer_encoded_bww_test, onehot_encoded_bww_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_words_string_test, integer_encoded_bww_test, onehot_encoded_bww_test = onehotencoding_betweenWords(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1pos n e2pos\n",
    "# train\n",
    "\n",
    "e1uniquepos = unique(e1pos_train)\n",
    "e2uniquepos = unique(e2pos_train)\n",
    "e1pos_string = list_to_string_original(e1pos_train)\n",
    "e2pos_string = list_to_string_original(e2pos_train)\n",
    "e1uniquepos_string = list_to_string(e1uniquepos)\n",
    "e2uniquepos_string = list_to_string(e2uniquepos)\n",
    "e1list_to_int, e1int_to_list = assign_indices(e1uniquepos_string)\n",
    "e2list_to_int, e2int_to_list = assign_indices(e2uniquepos_string)\n",
    "integer_encoded_e1 = [e1list_to_int[s] for s in e1pos_string]\n",
    "integer_encoded_e2 = [e2list_to_int[s] for s in e2pos_string]\n",
    "onehot_encoded_e1 = get_onehot_encoded(integer_encoded_e1, e1uniquepos_string)\n",
    "onehot_encoded_e2 = get_onehot_encoded(integer_encoded_e2, e2uniquepos_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1pos n e2pos\n",
    "# test\n",
    "\n",
    "def onehotencoding_e1e2pos(var1, var2):\n",
    "    e1pos_string_test = list_to_string_original(var1)\n",
    "    e2pos_string_test = list_to_string_original(var2)\n",
    "    integer_encoded_e1_test = get_integer_encoded(e1pos_string_test, e1list_to_int)\n",
    "    integer_encoded_e2_test = get_integer_encoded(e2pos_string_test, e2list_to_int)\n",
    "    onehot_encoded_e1_test = get_onehot_encoded(integer_encoded_e1_test, e1uniquepos_string)\n",
    "    onehot_encoded_e2_test = get_onehot_encoded(integer_encoded_e2_test, e2uniquepos_string)\n",
    "    return e1pos_string_test, e2pos_string_test, integer_encoded_e1_test, integer_encoded_e2_test, onehot_encoded_e1_test, onehot_encoded_e2_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1pos_string_test, e2pos_string_test, integer_encoded_e1_test, integer_encoded_e2_test, onehot_encoded_e1_test, onehot_encoded_e2_test = onehotencoding_e1e2pos(e1pos_test, e2pos_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of bwwords_pos\n",
    "# train\n",
    "\n",
    "bwwords_unique_pos = unique(bwwords_pos_train)\n",
    "bwwords_pos_string = list_to_string_original(bwwords_pos_train)\n",
    "bwwords_unique_pos_string = list_to_string(bwwords_unique_pos)\n",
    "bwwordslist_to_int, bwwordsint_to_list = assign_indices(bwwords_unique_pos_string)\n",
    "integer_encoded_bwwords = [bwwordslist_to_int[s] for s in bwwords_pos_string]\n",
    "onehot_encoded_bwwords = get_onehot_encoded(integer_encoded_bwwords, bwwords_unique_pos_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of bwwords_pos\n",
    "# test\n",
    "\n",
    "def onehotencoding_betweenwordspos(var1):\n",
    "    bwwords_pos_string_test = list_to_string_original(var1)\n",
    "    integer_encoded_bwwords_test = get_integer_encoded(bwwords_pos_string_test, bwwordslist_to_int)\n",
    "    onehot_encoded_bwwords_test = get_onehot_encoded(integer_encoded_bwwords_test, bwwords_unique_pos_string)\n",
    "    return bwwords_pos_string_test, integer_encoded_bwwords_test, onehot_encoded_bwwords_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwwords_pos_string_test, integer_encoded_bwwords_test, onehot_encoded_bwwords_test = onehotencoding_betweenwordspos(bwwords_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of before and after words of entities\n",
    "# train\n",
    "\n",
    "bef_aft_words = get_items(cr.inputList, 4)\n",
    "unique_bef_aft_words = unique(bef_aft_words)\n",
    "bef_aft_words_string = list_to_string_original(bef_aft_words)\n",
    "unique_bef_aft_words_string = list_to_string(unique_bef_aft_words)\n",
    "befaftlist_to_int, befaftint_to_list = assign_indices(unique_bef_aft_words_string)\n",
    "integer_encoded_befaft = [befaftlist_to_int[s] for s in bef_aft_words_string]\n",
    "onehot_encoded_befaft = get_onehot_encoded(integer_encoded_befaft, unique_bef_aft_words_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of before and after words of entities\n",
    "# test\n",
    "\n",
    "def onehotencoding_befaftwords(list1):\n",
    "    bef_aft_words_test = get_items(list1, 4)\n",
    "    bef_aft_words_string_test = list_to_string_original(bef_aft_words_test)\n",
    "    integer_encoded_befaft_test = get_integer_encoded(bef_aft_words_string_test, befaftlist_to_int)\n",
    "    onehot_encoded_befaft_test = get_onehot_encoded(integer_encoded_befaft_test, unique_bef_aft_words_string)\n",
    "    return bef_aft_words_test, bef_aft_words_string_test, integer_encoded_befaft_test, onehot_encoded_befaft_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_aft_words_test, bef_aft_words_string_test, integer_encoded_befaft_test, onehot_encoded_befaft_test = onehotencoding_befaftwords(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of noOfWords\n",
    "# train\n",
    "\n",
    "unique_noOfWords = unique(noOfWords_train)\n",
    "unique_noOfWords.append(\"*****\")\n",
    "noOfWords_list_to_int = dict((c, i) for i, c in enumerate(unique_noOfWords))\n",
    "noOfWords_int_to_list = dict((i, c) for i, c in enumerate(unique_noOfWords))\n",
    "integer_encoded_noOfWords = [noOfWords_list_to_int[c] for c in noOfWords_train]\n",
    "onehot_encoded_noOfWords = get_onehot_encoded(integer_encoded_noOfWords, unique_noOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of noOfWords\n",
    "# test\n",
    "\n",
    "def onehotencoding_noOfWords(var1):\n",
    "    integer_encoded_noOfWords_test = get_integer_encoded(var1, noOfWords_list_to_int)\n",
    "    onehot_encoded_noOfWords_test = get_onehot_encoded(integer_encoded_noOfWords_test, unique_noOfWords)\n",
    "    return integer_encoded_noOfWords_test, onehot_encoded_noOfWords_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded_noOfWords_test, onehot_encoded_noOfWords_test = onehotencoding_noOfWords(noOfWords_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1hypernyms and e2hypernyms\n",
    "# train\n",
    "\n",
    "e1unique_hyp = unique(e1hypernyms_train)\n",
    "e2unique_hyp = unique(e2hypernyms_train)\n",
    "e1hyp_string = hypernyms_to_string_original(e1hypernyms_train)\n",
    "e2hyp_string = hypernyms_to_string_original(e2hypernyms_train)\n",
    "e1unique_hyp_string = hypernyms_to_string(e1unique_hyp)\n",
    "e2unique_hyp_string = hypernyms_to_string(e2unique_hyp)\n",
    "e1hyplist_to_int, e1hypint_to_list = assign_indices(e1unique_hyp_string)\n",
    "e2hyplist_to_int, e2hypint_to_list = assign_indices(e2unique_hyp_string)\n",
    "integer_encoded_e1hyp = [e1hyplist_to_int[s] for s in e1hyp_string]\n",
    "integer_encoded_e2hyp = [e2hyplist_to_int[s] for s in e2hyp_string]\n",
    "onehot_encoded_e1hyp = get_onehot_encoded(integer_encoded_e1hyp, e1unique_hyp_string)\n",
    "onehot_encoded_e2hyp = get_onehot_encoded(integer_encoded_e2hyp, e2unique_hyp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1hypernyms and e2hypernyms\n",
    "# test\n",
    "\n",
    "def onehotencoding_e1e2hypernyms(var1, var2):\n",
    "    e1hyp_string_test = hypernyms_to_string_original(var1)\n",
    "    e2hyp_string_test = hypernyms_to_string_original(var2)\n",
    "    integer_encoded_e1hyp_test = get_integer_encoded(e1hyp_string_test, e1hyplist_to_int)\n",
    "    integer_encoded_e2hyp_test = get_integer_encoded(e2hyp_string_test, e2hyplist_to_int)\n",
    "    onehot_encoded_e1hyp_test = get_onehot_encoded(integer_encoded_e1hyp_test, e1unique_hyp_string)\n",
    "    onehot_encoded_e2hyp_test = get_onehot_encoded(integer_encoded_e2hyp_test, e2unique_hyp_string)\n",
    "    return e1hyp_string_test, e2hyp_string_test, integer_encoded_e1hyp_test, integer_encoded_e2hyp_test, onehot_encoded_e1hyp_test, onehot_encoded_e2hyp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1hyp_string_test, e2hyp_string_test, integer_encoded_e1hyp_test, integer_encoded_e2hyp_test, onehot_encoded_e1hyp_test, onehot_encoded_e2hyp_test = onehotencoding_e1e2hypernyms(e1hypernyms_test, e2hypernyms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1ner n e2ner\n",
    "# train\n",
    "\n",
    "e1unique_ner = unique(e1ner_train)\n",
    "e2unique_ner = unique(e2ner_train)\n",
    "e1ner_string = listoflist_to_string_original(e1ner_train)\n",
    "e2ner_string = listoflist_to_string_original(e2ner_train)\n",
    "e1unique_ner_string = listoflist_to_string(e1unique_ner)\n",
    "e2unique_ner_string = listoflist_to_string(e2unique_ner)\n",
    "e1nerlist_to_int, e1nerint_to_list = assign_indices(e1unique_ner_string)\n",
    "e2nerlist_to_int, e2nerint_to_list = assign_indices(e2unique_ner_string)\n",
    "integer_encoded_e1ner = [e1nerlist_to_int[s] for s in e1ner_string]\n",
    "integer_encoded_e2ner = [e2nerlist_to_int[s] for s in e2ner_string]\n",
    "onehot_encoded_e1ner = get_onehot_encoded(integer_encoded_e1ner, e1unique_ner_string)    \n",
    "onehot_encoded_e2ner = get_onehot_encoded(integer_encoded_e2ner, e2unique_ner_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of e1ner n e2ner\n",
    "# test\n",
    "\n",
    "def onehotencoding_e1e2ner(var1, var2):\n",
    "    e1ner_string_test = listoflist_to_string_original(var1)\n",
    "    e2ner_string_test = listoflist_to_string_original(var2)\n",
    "    integer_encoded_e1ner_test = get_integer_encoded(e1ner_string_test, e1nerlist_to_int)\n",
    "    integer_encoded_e2ner_test = get_integer_encoded(e2ner_string_test, e2nerlist_to_int)\n",
    "    onehot_encoded_e1ner_test = get_onehot_encoded(integer_encoded_e1ner_test, e1unique_ner_string)\n",
    "    onehot_encoded_e2ner_test = get_onehot_encoded(integer_encoded_e2ner_test, e2unique_ner_string)\n",
    "    return e1ner_string_test, e2ner_string_test, integer_encoded_e1ner_test, integer_encoded_e2ner_test, onehot_encoded_e1ner_test, onehot_encoded_e2ner_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1ner_string_test, e2ner_string_test, integer_encoded_e1ner_test, integer_encoded_e2ner_test, onehot_encoded_e1ner_test, onehot_encoded_e2ner_test = onehotencoding_e1e2ner(e1ner_test, e2ner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of direction of relation and relation name without direction and with direction\n",
    "# train\n",
    "\n",
    "def onehotencoding_ytrain(list1):\n",
    "    direction = get_items(list1, 5)\n",
    "    relation_name = get_items(list1, 6)\n",
    "    unique_relations = unique(relation_name)\n",
    "    unique_relations.append(\"*****\")\n",
    "    ysetlist_to_int_wo, ysetint_to_list_wo = assign_indices(unique_relations)\n",
    "    onehot_encoded_yset_wo = [ysetlist_to_int_wo[s] for s in relation_name]\n",
    "    yset = convert_to_string(direction, relation_name)\n",
    "    yset_unique = unique(yset)\n",
    "    yset_unique.append(\"*****\")\n",
    "    ysetlist_to_int_w, ysetint_to_list_w = assign_indices(yset_unique)\n",
    "    onehot_encoded_yset_w = [ysetlist_to_int_w[s] for s in yset]\n",
    "    return direction, relation_name, unique_relations, ysetlist_to_int_wo, ysetint_to_list_wo, onehot_encoded_yset_wo, yset, yset_unique, ysetlist_to_int_w, ysetint_to_list_w, onehot_encoded_yset_w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction, relation_name, unique_relations, ysetlist_to_int_wo, ysetint_to_list_wo, onehot_encoded_yset_wo, yset, yset_unique, ysetlist_to_int_w, ysetint_to_list_w, onehot_encoded_yset_w = onehotencoding_ytrain(cr.inputList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding of direction of relation and relation name without direction and with direction\n",
    "# test\n",
    "\n",
    "def onehotencoding_ytest(list1):\n",
    "    direction_test = get_items(list1, 5)\n",
    "    relation_name_test = get_items(list1, 6)\n",
    "    onehot_encoded_yset_test_wo = get_integer_encoded(relation_name_test, ysetlist_to_int_wo)\n",
    "    yset_test = convert_to_string(direction_test, relation_name_test)\n",
    "    onehot_encoded_yset_test_w = get_integer_encoded(yset_test, ysetlist_to_int_w)\n",
    "    return direction_test, relation_name_test, onehot_encoded_yset_test_wo, yset_test, onehot_encoded_yset_test_w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_test, relation_name_test, onehot_encoded_yset_test_wo, yset_test, onehot_encoded_yset_test_w = onehotencoding_ytest(cr.inputList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xw_train, xw_test\n",
    "\n",
    "xw_train = []\n",
    "\n",
    "for ind in range(len(cr.inputList)):\n",
    "    tmp = []\n",
    "    for x in onehot_encoded_e1words[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2words[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_bww[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_bwwords[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_befaft[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_noOfWords[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1hyp[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2hyp[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1ner[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1ner[ind]:\n",
    "        tmp.append(x)\n",
    "    xw_train.append(tmp)\n",
    "\n",
    "xw_test = []\n",
    "\n",
    "for ind in range(len(cr.inputList2)):\n",
    "    tmp = []\n",
    "    for x in onehot_encoded_e1words_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2words_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_bww_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_bwwords_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_befaft_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_noOfWords_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1hyp_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e2hyp_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1ner_test[ind]:\n",
    "        tmp.append(x)\n",
    "    for x in onehot_encoded_e1ner_test[ind]:\n",
    "        tmp.append(x)\n",
    "    xw_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "\n",
    "# X_train and X_test\n",
    "\n",
    "X_train = construct_csr(xw_train)\n",
    "X_test = construct_csr(xw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train and Y_test without considering direction\n",
    "\n",
    "Y_train = onehot_encoded_yset_wo\n",
    "Y_test = onehot_encoded_yset_test_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of our model without considering direction: 45.40381791483113 %\n",
      "\n",
      "Macro Precision, Recall and F-scores and Support of our model without considering direction:\n",
      "\n",
      "(array([0.33333333, 0.56267409, 0.51754386, 0.47058824, 0.605     ,\n",
      "       0.3566879 , 0.45578231, 0.60747664, 0.2114094 , 0.42447917,\n",
      "       0.60215054, 0.89380531, 0.52083333, 0.71794872, 0.52      ,\n",
      "       0.60465116, 0.52941176, 0.74193548]), array([0.60387324, 0.66229508, 0.36875   , 0.1       , 0.53070175,\n",
      "       0.29015544, 0.536     , 0.37572254, 0.150358  , 0.61278195,\n",
      "       0.44444444, 0.73722628, 0.18382353, 0.78504673, 0.30409357,\n",
      "       0.45614035, 0.1011236 , 0.70769231]), array([0.42955542, 0.60843373, 0.43065693, 0.16494845, 0.56542056,\n",
      "       0.32      , 0.49264706, 0.46428571, 0.17573222, 0.50153846,\n",
      "       0.51141553, 0.808     , 0.27173913, 0.75      , 0.38376384,\n",
      "       0.52      , 0.16981132, 0.72440945]), array([568, 305, 160,  80, 228, 193, 125, 173, 419, 266, 126, 137, 136,\n",
      "       107, 171,  57,  89,  65], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier without considering direction\n",
    "\n",
    "mdl_wo = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, Y_train)\n",
    "mdl_wo.fit(X_train, Y_train)\n",
    "accuracy_mdl_wo = mdl_wo.score(X_test, Y_test)\n",
    "print(f\"\\nAccuracy of our model without considering direction: {accuracy_mdl_wo * 100} %\")\n",
    "y_pred_wo = mdl_wo.predict(X_test)\n",
    "print(\"\\nMacro Precision, Recall and F-scores and Support of our model without considering direction:\\n\")\n",
    "print(precision_recall_fscore_support(Y_test, y_pred_wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train and Y_test considering direction\n",
    "\n",
    "Y_train = onehot_encoded_yset_w\n",
    "Y_test = onehot_encoded_yset_test_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of our model considering direction: 44.22907488986784 %\n",
      "\n",
      "Macro Precision, Recall and F-scores and Support of our model considering direction:\n",
      "\n",
      "(array([0.34615385, 0.54285714, 0.58333333, 0.33333333, 0.36266667,\n",
      "       0.53932584, 0.38095238, 0.28767123, 0.42553191, 0.57142857,\n",
      "       0.21212121, 0.33082707, 0.5       , 0.91891892, 0.5       ,\n",
      "       0.89361702, 0.7721519 , 0.48076923, 0.67123288, 0.46666667,\n",
      "       0.45851528, 0.44680851, 0.59090909, 0.60683761, 0.42857143,\n",
      "       0.22222222, 0.52941176, 0.64285714, 0.        , 0.625     ,\n",
      "       0.35820896, 0.45762712, 0.63636364, 0.75      , 0.84      ]), array([0.5591716 , 0.74347826, 0.4       , 0.16666667, 0.59130435,\n",
      "       0.5106383 , 0.14545455, 0.23333333, 0.49382716, 0.36363636,\n",
      "       0.23389021, 0.45360825, 0.39393939, 0.86075949, 0.16901408,\n",
      "       0.72413793, 0.89705882, 0.27472527, 0.52688172, 0.23333333,\n",
      "       0.62130178, 0.47727273, 0.2       , 0.52985075, 0.15      ,\n",
      "       0.05263158, 0.33333333, 0.28571429, 0.        , 0.16393443,\n",
      "       0.23300971, 0.36      , 0.71794872, 0.75      , 0.63636364]), array([0.42760181, 0.62752294, 0.47457627, 0.22222222, 0.44958678,\n",
      "       0.52459016, 0.21052632, 0.25766871, 0.45714286, 0.44444444,\n",
      "       0.22247446, 0.3826087 , 0.44067797, 0.88888889, 0.25263158,\n",
      "       0.8       , 0.82993197, 0.34965035, 0.59036145, 0.31111111,\n",
      "       0.52763819, 0.46153846, 0.29885057, 0.56573705, 0.22222222,\n",
      "       0.08510638, 0.40909091, 0.3956044 , 0.        , 0.25974026,\n",
      "       0.28235294, 0.40298507, 0.6746988 , 0.75      , 0.72413793]), array([338, 230, 105,  42, 230,  94,  55,  90,  81, 110, 419,  97,  33,\n",
      "        79,  71,  58,  68,  91,  93,  30, 169,  44,  65, 134,  80,  38,\n",
      "        27,  63,  28,  61, 103,  75,  39,  32,  33], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier considering direction\n",
    "\n",
    "mdl_w = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, Y_train)\n",
    "mdl_w.fit(X_train, Y_train)\n",
    "accuracy_mdl_w = mdl_w.score(X_test, Y_test)\n",
    "print(f\"\\nAccuracy of our model considering direction: {accuracy_mdl_w * 100} %\")\n",
    "y_pred_w = mdl_w.predict(X_test)\n",
    "print(\"\\nMacro Precision, Recall and F-scores and Support of our model considering direction:\\n\")\n",
    "print(precision_recall_fscore_support(Y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a valid test sentence - \" De <e1> La Salle High School </e1> was founded by the <e2> Christian Brothers </e2> . \"\" <e1> Tunisia </e1> 's flag carrier <e2> Tunisair </e2> scored 60 million Tunisian dinars ( 41.15 million US dollars ) in net income in 2009 despite world economic downturn which badly hit air transport industry the company said on Friday . \"\n",
      "\n",
      "We've stored the test sentence in the following form:\n",
      "\n",
      "[[\"Tunisia 's flag carrier Tunisair scored 60 million Tunisian dinars ( 41.15 million US dollars ) in net income in 2009 despite world economic downturn which badly hit air transport industry the company said on Friday .\", 'Tunisia', 'Tunisair', \"'s flag carrier\", ['', 'scored']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = input(\"Please enter a valid test sentence - \\\" De <e1> La Salle High School </e1> was founded by the <e2> Christian Brothers </e2> . \\\"\")\n",
    "testList = createlist_ts(test_sentence)\n",
    "print(\"\\nWe've stored the test sentence in the following form:\")\n",
    "print(f\"\\n{testList}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tokens of entity e1 are:\n",
      "\n",
      "[['Tunisia']]\n",
      "\n",
      "\n",
      "The tokens of entity e2 are:\n",
      "\n",
      "[['Tunisair']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1Tokens_test_sentence, e2Tokens_test_sentence = entities_tokens(testList)\n",
    "print(\"\\nThe tokens of entity e1 are:\")\n",
    "print(f\"\\n{e1Tokens_test_sentence}\\n\")\n",
    "print(\"\\nThe tokens of entity e2 are:\")\n",
    "print(f\"\\n{e2Tokens_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The lemmas in the sentence are:\n",
      "\n",
      "[['Tunisia', \"'s\", 'flag', 'carrier', 'Tunisair', 'scored', '60', 'million', 'Tunisian', 'dinar', '(', '41.15', 'million', 'US', 'dollar', ')', 'in', 'net', 'income', 'in', '2009', 'despite', 'world', 'economic', 'downturn', 'which', 'badly', 'hit', 'air', 'transport', 'industry', 'the', 'company', 'said', 'on', 'Friday', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmas_test_sentence = lemmas_sentences(testList)\n",
    "print(\"\\nThe lemmas in the sentence are:\")\n",
    "print(f\"\\n{lemmas_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pos tags in the sentence are:\n",
      "\n",
      "[['NNP', 'POS', 'NN', 'NN', 'NNP', 'VBD', 'CD', 'CD', 'JJ', 'NNS', '(', 'CD', 'CD', 'NNP', 'NNS', ')', 'IN', 'JJ', 'NN', 'IN', 'CD', 'IN', 'NN', 'JJ', 'NN', 'WDT', 'RB', 'VBD', 'NN', 'NN', 'NN', 'DT', 'NN', 'VBD', 'IN', 'NNP', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_test_sentence = pos_sentences(testList)\n",
    "print(\"\\nThe pos tags in the sentence are:\")\n",
    "print(f\"\\n{pos_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dependencies in the sentence are:\n",
      "\n",
      "[[['poss', 'carrier'], ['case', 'Tunisia'], ['compound', 'carrier'], ['nsubj', 'scored'], ['nsubj', 'scored'], ['ROOT', 'scored'], ['compound', 'million'], ['nummod', 'dinars'], ['amod', 'dinars'], ['dobj', 'scored'], ['punct', 'dinars'], ['compound', 'million'], ['nummod', 'dollars'], ['compound', 'dollars'], ['appos', 'dinars'], ['punct', 'dinars'], ['prep', 'scored'], ['amod', 'income'], ['pobj', 'in'], ['prep', 'scored'], ['pobj', 'in'], ['prep', 'scored'], ['nmod', 'downturn'], ['amod', 'downturn'], ['pobj', 'despite'], ['nsubj', 'hit'], ['advmod', 'hit'], ['relcl', 'downturn'], ['compound', 'transport'], ['compound', 'industry'], ['dobj', 'hit'], ['det', 'company'], ['nsubj', 'said'], ['relcl', 'downturn'], ['prep', 'said'], ['pobj', 'on'], ['punct', 'scored']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deps_test_sentence = deps_sentences(testList)\n",
    "print(\"\\nThe dependencies in the sentence are:\")\n",
    "print(f\"\\n{deps_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hypernyms in the sentence are:\n",
      "\n",
      "[[Synset('list.n.01'), Synset('organism.n.01'), Synset('notch.v.01'), Synset('large_integer.n.01'), Synset('large_integer.n.01'), Synset('african.n.01'), Synset('tunisian_monetary_unit.n.01'), Synset('large_integer.n.01'), Synset('monetary_unit.n.01'), Synset('metallic_element.n.01'), Synset('income.n.01'), Synset('financial_gain.n.01'), Synset('metallic_element.n.01'), Synset('disregard.n.02'), Synset('worsening.n.02'), Synset('succeed.v.01'), Synset('region.n.01'), Synset('move.v.02'), Synset('commercial_enterprise.n.01'), Synset('complement.n.03'), Synset('express.v.02'), Synset('weekday.n.01')]]\n",
      "\n",
      "\n",
      "The hyponyms in the sentence are:\n",
      "\n",
      "[[Synset('line.v.04'), Synset('scarify.v.02'), Synset('scotch.v.02'), Synset('scribe.v.01'), Synset('australian_dollar.n.01'), Synset('bahamian_dollar.n.01'), Synset('barbados_dollar.n.01'), Synset('belize_dollar.n.01'), Synset('bermuda_dollar.n.01'), Synset('brunei_dollar.n.01'), Synset('canadian_dollar.n.01'), Synset('cayman_islands_dollar.n.01'), Synset('dominican_dollar.n.01'), Synset('fiji_dollar.n.01'), Synset('grenada_dollar.n.01'), Synset('guyana_dollar.n.01'), Synset('hong_kong_dollar.n.01'), Synset('jamaican_dollar.n.01'), Synset('kiribati_dollar.n.01'), Synset('liberian_dollar.n.01'), Synset('new_zealand_dollar.n.01'), Synset('singapore_dollar.n.01'), Synset('taiwan_dollar.n.01'), Synset('trinidad_and_tobago_dollar.n.01'), Synset('tuvalu_dollar.n.01'), Synset('united_states_dollar.n.01'), Synset('zimbabwean_dollar.n.01'), Synset('accumulation.n.04'), Synset('dividend.n.01'), Synset('earning_per_share.n.01'), Synset('fast_buck.n.01'), Synset('filthy_lucre.n.01'), Synset('gross_profit.n.01'), Synset('killing.n.03'), Synset('markup.n.01'), Synset('windfall_profit.n.01'), Synset('cash_flow.n.01'), Synset('disposable_income.n.01'), Synset('double_dipping.n.01'), Synset('easy_money.n.01'), Synset('ebitda.n.01'), Synset('government_income.n.01'), Synset('gross_sales.n.01'), Synset('net_income.n.01'), Synset('net_sales.n.01'), Synset('per_capita_income.n.01'), Synset('personal_income.n.01'), Synset('rental_income.n.01'), Synset('return.n.06'), Synset('unearned_income.n.01'), Synset('unearned_income.n.02'), Synset('downspin.n.01'), Synset('airspace.n.02'), Synset('bear.v.04'), Synset('bring.v.01'), Synset('bucket.v.02'), Synset('cart.v.02'), Synset('chariot.v.01'), Synset('fly.v.04'), Synset('haul.v.02'), Synset('lug.v.01'), Synset('pack.v.04'), Synset('pipe_in.v.01'), Synset('port.v.05'), Synset('port.v.06'), Synset('porter.v.01'), Synset('return.v.07'), Synset('shoulder.v.03'), Synset('aluminum_business.n.01'), Synset('apparel_industry.n.01'), Synset('automobile_industry.n.01'), Synset('aviation.n.02'), Synset('banking_industry.n.01'), Synset('chemical_industry.n.01'), Synset('coal_industry.n.01'), Synset('computer_industry.n.01'), Synset('construction_industry.n.01'), Synset('electronics_industry.n.01'), Synset('entertainment_industry.n.01'), Synset('film_industry.n.01'), Synset('growth_industry.n.01'), Synset('lighting_industry.n.01'), Synset('market.n.04'), Synset('munitions_industry.n.01'), Synset('oil_industry.n.01'), Synset('plastics_industry.n.01'), Synset('service_industry.n.01'), Synset('shipbuilding_industry.n.01'), Synset('shoe_industry.n.01'), Synset('sign_industry.n.01'), Synset('steel_industry.n.01'), Synset('sunrise_industry.n.01'), Synset('tobacco_industry.n.01'), Synset('toy_industry.n.01'), Synset('trucking_industry.n.01'), Synset('add.v.02'), Synset('announce.v.02'), Synset('answer.v.01'), Synset('articulate.v.05'), Synset('declare.v.01'), Synset('declare.v.07'), Synset('explain.v.02'), Synset('get_out.v.04'), Synset('give.v.04'), Synset('misstate.v.01'), Synset('note.v.01'), Synset('precede.v.05'), Synset('present.v.02'), Synset('summarize.v.02')]]\n",
      "\n",
      "\n",
      "The meronyms in the sentence are:\n",
      "\n",
      "[[Synset('ariana.n.01'), Synset('atlas_mountains.n.01'), Synset('ehadhamen.n.01'), Synset('gafsa.n.01'), Synset('sfax.n.01'), Synset('sousse.n.01'), Synset('tunis.n.01'), Synset('tunisian_dirham.n.01'), Synset('alabama.n.01'), Synset('alaska.n.01'), Synset('american_state.n.01'), Synset('arizona.n.01'), Synset('arkansas.n.01'), Synset('california.n.01'), Synset('colony.n.03'), Synset('colorado.n.01'), Synset('connecticut.n.01'), Synset('connecticut.n.02'), Synset('dakota.n.02'), Synset('delaware.n.04'), Synset('district_of_columbia.n.01'), Synset('east.n.03'), Synset('florida.n.01'), Synset('georgia.n.01'), Synset('great_lakes.n.01'), Synset('hawaii.n.01'), Synset('idaho.n.01'), Synset('illinois.n.01'), Synset('indiana.n.01'), Synset('iowa.n.02'), Synset('kansas.n.01'), Synset('kentucky.n.01'), Synset('louisiana.n.01'), Synset('louisiana_purchase.n.01'), Synset('maine.n.01'), Synset('maryland.n.01'), Synset('massachusetts.n.01'), Synset('michigan.n.01'), Synset('mid-atlantic_states.n.01'), Synset('midwest.n.01'), Synset('minnesota.n.01'), Synset('mississippi.n.01'), Synset('mississippi.n.02'), Synset('missouri.n.01'), Synset('missouri.n.02'), Synset('montana.n.01'), Synset('nebraska.n.01'), Synset('nevada.n.01'), Synset('new_england.n.01'), Synset('new_hampshire.n.01'), Synset('new_jersey.n.01'), Synset('new_mexico.n.01'), Synset('new_river.n.01'), Synset('new_york.n.02'), Synset('niagara.n.02'), Synset('niobrara.n.01'), Synset('north.n.01'), Synset('north_carolina.n.01'), Synset('north_dakota.n.01'), Synset('ohio.n.01'), Synset('ohio.n.02'), Synset('oklahoma.n.01'), Synset('oregon.n.01'), Synset('pacific_northwest.n.01'), Synset('pennsylvania.n.01'), Synset('rhode_island.n.01'), Synset('rio_grande.n.01'), Synset('saint_lawrence.n.02'), Synset('south.n.01'), Synset('south_carolina.n.02'), Synset('south_dakota.n.01'), Synset('sunbelt.n.01'), Synset('tennessee.n.01'), Synset('texas.n.01'), Synset('twin.n.03'), Synset('utah.n.01'), Synset('vermont.n.01'), Synset('virginia.n.01'), Synset('washington.n.02'), Synset('west.n.03'), Synset('west_virginia.n.01'), Synset('wisconsin.n.02'), Synset('wyoming.n.01'), Synset('yosemite.n.01'), Synset('yukon.n.01'), Synset('cent.n.01'), Synset('share.n.01'), Synset('airspace.n.01'), Synset('ionosphere.n.01')]]\n",
      "\n",
      "\n",
      "The holonyms in the sentence are:\n",
      "\n",
      "[[Synset('maghreb.n.01'), Synset('north_america.n.01'), Synset('earth.n.01')]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypernyms_test_sentence, hyponyms_test_sentence, meronyms_test_sentence, holonyms_test_sentence = hhmh_sentences(testList)\n",
    "print(\"\\nThe hypernyms in the sentence are:\")\n",
    "print(f\"\\n{hypernyms_test_sentence}\\n\")\n",
    "print(\"\\nThe hyponyms in the sentence are:\")\n",
    "print(f\"\\n{hyponyms_test_sentence}\\n\")\n",
    "print(\"\\nThe meronyms in the sentence are:\")\n",
    "print(f\"\\n{meronyms_test_sentence}\\n\")\n",
    "print(\"\\nThe holonyms in the sentence are:\")\n",
    "print(f\"\\n{holonyms_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The NER tags of entity e1 are:\n",
      "\n",
      "[[]]\n",
      "\n",
      "\n",
      "The NER tags of entity e2 are:\n",
      "\n",
      "[[]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1ner_test_sentence, e2ner_test_sentence = e1e2_ner(testList)\n",
    "print(\"\\nThe NER tags of entity e1 are:\")\n",
    "print(f\"\\n{e1ner_test_sentence}\\n\")\n",
    "print(\"\\nThe NER tags of entity e2 are:\")\n",
    "print(f\"\\n{e2ner_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The POS tags of tokens of entity e1 are:\n",
      "\n",
      "[['NN']]\n",
      "\n",
      "\n",
      "The POS tags of tokens of entity e2 are:\n",
      "\n",
      "[['NN']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1pos_test_sentence, e2pos_test_sentence = e1pos_e2pos(e1Tokens_test_sentence, e2Tokens_test_sentence)\n",
    "print(\"\\nThe POS tags of tokens of entity e1 are:\")\n",
    "print(f\"\\n{e1pos_test_sentence}\\n\")\n",
    "print(\"\\nThe POS tags of tokens of entity e2 are:\")\n",
    "print(f\"\\n{e2pos_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The POS tags of tokens between entities e1 and e2 are:\n",
      "\n",
      "[['POS', 'NN', 'NN']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bwwords_pos_test_sentence = bwwords_pos_entities(testList)\n",
    "print(\"\\nThe POS tags of tokens between entities e1 and e2 are:\")\n",
    "print(f\"\\n{bwwords_pos_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of words between entities e1 and e2 is:\n",
      "\n",
      "[3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noOfWords_test_sentence = noOfWords_entities(testList)\n",
    "print(\"\\nThe number of words between entities e1 and e2 is:\")\n",
    "print(f\"\\n{noOfWords_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hypernyms of entity e1 in the sentence are:\n",
      "\n",
      "[[]]\n",
      "\n",
      "\n",
      "The hypernyms of entity e2 in the sentence are:\n",
      "\n",
      "[[]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1hypernyms_test_sentence, e2hypernyms_test_sentence = e1e2_hypernyms(testList)\n",
    "print(\"\\nThe hypernyms of entity e1 in the sentence are:\")\n",
    "print(f\"\\n{e1hypernyms_test_sentence}\\n\")\n",
    "print(\"\\nThe hypernyms of entity e2 in the sentence are:\")\n",
    "print(f\"\\n{e2hypernyms_test_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_string_test_sentence, e2_string_test_sentence, integer_encoded_e1words_test_sentence, integer_encoded_e2words_test_sentence, onehot_encoded_e1words_test_sentence, onehot_encoded_e2words_test_sentence = onehotencoding_e1e2Words(testList)\n",
    "bw_words_string_test_sentence, integer_encoded_bww_test_sentence, onehot_encoded_bww_test_sentence = onehotencoding_betweenWords(testList)\n",
    "e1pos_string_test_sentence, e2pos_string_test_sentence, integer_encoded_e1_test_sentence, integer_encoded_e2_test_sentence, onehot_encoded_e1_test_sentence, onehot_encoded_e2_test_sentence = onehotencoding_e1e2pos(e1pos_test_sentence, e2pos_test_sentence) \n",
    "bwwords_pos_string_test_sentence, integer_encoded_bwwords_test_sentence, onehot_encoded_bwwords_test_sentence = onehotencoding_betweenwordspos(bwwords_pos_test_sentence)\n",
    "bef_aft_words_test_sentence, bef_aft_words_string_test_sentence, integer_encoded_befaft_test_sentence, onehot_encoded_befaft_test_sentence = onehotencoding_befaftwords(testList)\n",
    "integer_encoded_noOfWords_test_sentence, onehot_encoded_noOfWords_test_sentence = onehotencoding_noOfWords(noOfWords_test_sentence)\n",
    "e1hyp_string_test_sentence, e2hyp_string_test_sentence, integer_encoded_e1hyp_test_sentence, integer_encoded_e2hyp_test_sentence, onehot_encoded_e1hyp_test_sentence, onehot_encoded_e2hyp_test_sentence = onehotencoding_e1e2hypernyms(e1hypernyms_test_sentence, e2hypernyms_test_sentence)\n",
    "e1ner_string_test_sentence, e2ner_string_test_sentence, integer_encoded_e1ner_test_sentence, integer_encoded_e2ner_test_sentence, onehot_encoded_e1ner_test_sentence, onehot_encoded_e2ner_test_sentence = onehotencoding_e1e2ner(e1ner_test_sentence, e2ner_test_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_sentence\n",
    "\n",
    "xw_ts = flatten_test_sentence()\n",
    "X_test_sentence = construct_csr(xw_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relation between entities in the test sentence is country_of_headquarters(e2, e1)\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "Y_test_sent_pred = mdl_w.predict(X_test_sentence)\n",
    "feat_name = ysetint_to_list_w.get(Y_test_sent_pred[0])\n",
    "predict_y(feat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
